{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Asisatant with web knowledge\n",
    "\n",
    "![personal Assistant PoC](images/nvidia_personal_assistant.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade --quiet langchain-nvidia-ai-endpoints\n",
    "#!pip install langchain==0.1.13 langchain-community==0.0.31 langchain-core==0.1.38\n",
    "#!pip install beautifulsoup4==4.12.3 numpy==1.26.4 sounddevice==0.4.6 openai-whisper==20231117 rich==13.7.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Tech: Nvidia + LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain-nvidia-ai-endpoints\n",
    "\n",
    "The langchain-nvidia-ai-endpoints package contains LangChain integrations building applications with models on NVIDIA NIM inference microservice. NVIDIA hosted deployments of NIMs are available to test on the [NVIDIA API catalog](https://build.nvidia.com/explore/discover). \n",
    "\n",
    "NVIDIA NIM supports models across domains like chat, embedding, and re-ranking models from the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA accelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single command on NVIDIA accelerated infrastructure.\n",
    "\n",
    "For more information on accessing the chat models through this api, check out the [ChatNVIDIA](https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation models through the NVIDIA NIM APIs or endpoints\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# along with the LangChain framework\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App & GUI Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for allication and GUI\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import subprocess\n",
    "from queue import Queue\n",
    "from rich.console import Console\n",
    "\n",
    "# for speech recognition\n",
    "import whisper\n",
    "\n",
    "# for search result scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup NVIDIA NIM endpoint\n",
    "\n",
    "1. Create a free account with NVIDIA, which hosts NVIDIA AI Foundation models.\n",
    "1. Click on your model of choice.\n",
    "1. Under Input select the Python tab, and click Get API Key. Then click Generate Key.\n",
    "1. Copy and save the generated key as NVIDIA_API_KEY. From there, you should have access to the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get access to the NVIDIA API\n",
    "import getpass\n",
    "import os\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM on Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with NVIDIA API Catalog\n",
    "chat = ChatNVIDIA(\n",
    "    model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=100,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n",
    "# if you have Nvidia hardware\n",
    "# Working with NVIDIA NIMs, as fully local computing source\n",
    "# connect to an embedding NIM running at localhost:8000, specifying a specific model\n",
    "# chat = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama3-8b-instruct\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain ConversationChain\n",
    "\n",
    "ConversationChain to have a conversation and load context from memory. Key Features of LangChain's ConversationChain: \n",
    "1. Context Management: Tracks the history of the conversation, allowing the AI to provide contextually relevant responses.\n",
    "1. Memory: Stores important information across multiple turns, such as user preferences or key details mentioned earlier in the conversation.\n",
    "1. Response Generation: Integrates with various language models to generate natural language responses.\n",
    "1. State Management: Maintains the overall state of the conversation, keeping track of ongoing topics, user queries, and other relevant details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful and friendly AI assistant. You are polite, respectful, and aim to provide concise responses of less \n",
    "than 20 words.\n",
    "\n",
    "The conversation transcript is as follows:\n",
    "{history}\n",
    "\n",
    "And here is the user's follow-up: {input}\n",
    "\n",
    "Your response:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "chain = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Assistant:\"),\n",
    "    llm=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App & GUI\n",
    "\n",
    "This voice assistant demo leverages advanced Nvidia technology and the LangChain framework. Key features include:\n",
    "\n",
    "1. Nvidia NIM Computing Power: Serves as the BRAIN, providing robust computational capabilities.\n",
    "1. LangChain Framework Coordination: Functions as the NERVE, seamlessly managing and integrating various components of the system.\n",
    "1. Edge Device Compatibility: Supports input and output through voice or text on any edge device, such as AI-enabled PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current Demo Performance and Future Improvements\n",
    "\n",
    "The current demonstration experiences significant delays in processing both input and output. For input processing, the OpenAI Whisper speech-to-text model is executed on a MacBook. For output, the default Mac text-to-speech application, Say, is utilized. To achieve real-time performance, the following improvement plans are proposed:\n",
    "\n",
    "* Hardware Upgrade with Nvidia GPUs: \n",
    "   1. Deploy Nvidia NIMs locally to utilize self-hosted models\n",
    "   1. Implement with NVIDIA TensorRT-LLM for superior text-to-speech models, enabling customized voice options\n",
    "* Multilingual input & output: Integrate Nvidia's technology to support multilingual capabilities using various models, such as those available on Huggingface\n",
    "* Enhanced Answer Accuracy: Utilize LangGraph to incorporate AI Agent technology, thereby improving the accuracy of responses\n",
    "\n",
    "These enhancements aim to significantly reduce processing times and improve the overall efficiency and effectiveness of the demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "stt = whisper.load_model(\"base.en\") \n",
    "\n",
    "def record_audio(stop_event, data_queue):\n",
    "    \"\"\"\n",
    "    Captures audio data from the user's microphone and adds it to a queue for further processing.\n",
    "\n",
    "    Args:\n",
    "        stop_event (threading.Event): An event that, when set, signals the function to stop recording.\n",
    "        data_queue (queue.Queue): A queue to which the recorded audio data will be added.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            console.print(status)\n",
    "        data_queue.put(bytes(indata))\n",
    "\n",
    "    with sd.RawInputStream(\n",
    "        samplerate=16000, dtype=\"int16\", channels=1, callback=callback\n",
    "    ):\n",
    "        while not stop_event.is_set():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "\n",
    "def transcribe(audio_np: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes the given audio data using the Whisper speech recognition model.\n",
    "\n",
    "    Args:\n",
    "        audio_np (numpy.ndarray): The audio data to be transcribed.\n",
    "\n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    result = stt.transcribe(audio_np, fp16=False)  # Set fp16=True if using a GPU\n",
    "    text = result[\"text\"].strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_llm_response(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response to the given text using the Nvidia NIM's open language model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response.\n",
    "    \"\"\"\n",
    "    response = chain.predict(input=text)\n",
    "    if response.startswith(\"Assistant:\"):\n",
    "        response = response[len(\"Assistant:\") :].strip()\n",
    "    return response\n",
    "\n",
    "# define a function to fetch srp\n",
    "def fetch_srp(query):\n",
    "    # fetch an restful API\n",
    "    # print('Fetching srp by query')\n",
    "    r = requests.get(f\"https://www.google.com/search?q={query}&hl=en&lr=lang_en\")\n",
    "    #r.content\n",
    "\n",
    "    # Use the 'html.parser' to parse the page\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')  \n",
    "    text = soup.get_text()\n",
    "    #print(text)\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    console.print(\"[cyan]Assistant started! Press Ctrl+C to exit.\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            console.input(\n",
    "                \"Press Enter to start recording, then press Enter again to stop.\"\n",
    "            )\n",
    "\n",
    "            data_queue = Queue()  # type: ignore[var-annotated]\n",
    "            stop_event = threading.Event()\n",
    "            recording_thread = threading.Thread(\n",
    "                target=record_audio,\n",
    "                args=(stop_event, data_queue),\n",
    "            )\n",
    "            recording_thread.start()\n",
    "\n",
    "            input()\n",
    "            stop_event.set()\n",
    "            recording_thread.join()\n",
    "\n",
    "            audio_data = b\"\".join(list(data_queue.queue))\n",
    "            audio_np = (\n",
    "                np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            )\n",
    "\n",
    "            if audio_np.size > 0:\n",
    "                with console.status(\"Transcribing...\", spinner=\"earth\"):\n",
    "                    text = transcribe(audio_np)\n",
    "                console.print(f\"[yellow]You: {text}\")\n",
    "\n",
    "                with console.status(\"Generating response...\", spinner=\"earth\"):\n",
    "                    srp = fetch_srp(text)\n",
    "                    #append the srp to the text\n",
    "                    text = text + \". \\n\\nAnd here is the related search result snippets for you to prepare response: \" + srp\n",
    "                    response = get_llm_response(text)\n",
    "                    #sample_rate, audio_array = tts.long_form_synthesize(response)\n",
    "\n",
    "                console.print(f\"[cyan]Assistant: {response}\")\n",
    "                #play_audio(sample_rate, audio_array)\n",
    "                subprocess.run([\"say\", response])\n",
    "            else:\n",
    "                console.print(\n",
    "                    \"[red]No audio recorded. Please ensure your microphone is working.\"\n",
    "                )\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        console.print(\"\\n[red]Exiting...\")\n",
    "\n",
    "    console.print(\"[blue]Session ended.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "* Thanks DUY HUYNH. The voice processing & console UI code is copied from DUY's work [Build your own voice assistant and run it locally: Whisper + Ollama + Bark](https://blog.duy.dev/build-your-own-voice-assistant-and-run-it-locally/)\n",
    "* Nvidia for free NIMs endpoint credits\n",
    "* LangChain for lots of tutorials  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the app\n",
    "\n",
    "```shell:\n",
    "python app_nvidia.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
